{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5e360-dab8-4e0b-a20c-4b23bfa70e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ba0d7d-078d-4482-bad4-b41d9a900978",
   "metadata": {},
   "source": [
    "# CS74 Final Project\n",
    "### Kevin King, Spring 2022\n",
    "### Due: Tuesday, June 7th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc0cc3-9742-4957-93dd-aa0f0dc42f15",
   "metadata": {},
   "source": [
    "Import libraries needed for the final project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9834c6-94d4-40a2-9c57-fb574a39aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a1cd1-b6a3-4354-a992-6a24354bdaea",
   "metadata": {},
   "source": [
    "Import training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef9215b-e342-4988-b5c5-8108016e4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = pd.read_table('Training.csv', delimiter=',').fillna('NULL')\n",
    "test_table = pd.read_table('Test.csv', delimiter=',').fillna('NULL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df0d6e-7b90-4438-92ab-dbb86c6458ae",
   "metadata": {},
   "source": [
    "### Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb5c049-f208-48aa-a613-0886729327c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the predictions to a csv file for Kaggle\n",
    "def to_csv(y_pred, type):\n",
    "    type_string = \"\"\n",
    "    if type == \"binary\":\n",
    "        type_string = \"binary_class\"\n",
    "    elif type == \"multiclass\":\n",
    "        type_string = \"multiclass\"\n",
    "        \n",
    "    out = {'id': range(len(y_pred)), 'predicted': y_pred}\n",
    "    outdf = pd.DataFrame(out)\n",
    "    outdf.to_csv(f'{type_string}_{cutoff}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e5dbfb-c80b-4b4b-ba0a-2842dadb6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# train data: y = classes, x = features\n",
    "train_df = pd.DataFrame(train_table, columns = ['overall', 'reviewText','category', 'verified'])\n",
    "y_train_df = train_df[['overall']].copy().astype(float)\n",
    "x_train_df = train_df[['reviewText', 'category', 'verified']].copy()\n",
    "\n",
    "# test data: x = features\n",
    "test_df = pd.DataFrame(test_table, columns = ['overall', 'reviewText','category', 'verified'])\n",
    "x_test_df = test_df[['reviewText', 'category', 'verified']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf84c89-d1d4-493a-9a35-763d76a5c40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Average (k-fold): 0.5916945263872193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Accuracy': [0.696985268927715,\n",
       "   0.434737923946557,\n",
       "   0.8247687564234327,\n",
       "   0.8538883179170949,\n",
       "   0.8600308377591228],\n",
       "  'Precision': [0.6726949259939236,\n",
       "   0.4035637177150162,\n",
       "   0.6691966644563426,\n",
       "   0.7530204652905287,\n",
       "   0.7588472530227138],\n",
       "  'Recall': [0.5902247725112046,\n",
       "   0.33106465515096317,\n",
       "   0.6183296150019597,\n",
       "   0.654327346560965,\n",
       "   0.6909403992098928],\n",
       "  'F1': [0.5827172566536343,\n",
       "   0.34196980740763006,\n",
       "   0.6347171141676222,\n",
       "   0.6832384720099887,\n",
       "   0.7158299816972216]},\n",
       " array([1, 1, 1, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# vectorizes train and test features - words to numbers\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_features = vectorizer.fit_transform(x_train_df.reviewText.tolist())\n",
    "test_features = vectorizer.transform(x_test_df.reviewText.tolist())\n",
    "\n",
    "def construct_labels(cutoff):\n",
    "    y_label = y_train_df.overall.copy()\n",
    "    y_label.loc[y_train_df.overall > cutoff] = 1\n",
    "    y_label.loc[y_train_df.overall <= cutoff] = 0\n",
    "    return y_label\n",
    "\n",
    "def evaluate(y_test, y_pred):\n",
    "    assert len(y_test) == len(y_pred), 'labels array and predictions array must be the same length'\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def binary_classifier(cutoff, submission):\n",
    "    kf = KFold(n_splits=5) # change n_splits?\n",
    "    \n",
    "    # results dictionary\n",
    "    results = {'Accuracy' : [],\n",
    "               'Precision' : [],\n",
    "               'Recall' : [],\n",
    "               'F1' : []}\n",
    "    \n",
    "    train_labels = construct_labels(cutoff)\n",
    "    model = LogisticRegression(fit_intercept=False)\n",
    "    \n",
    "    if submission == True:\n",
    "        model.fit(train_features, train_labels)\n",
    "        y_pred = model.predict(test_features).astype(int)\n",
    "    \n",
    "    elif submission == False:\n",
    "        for train_index, test_index in kf.split(train_features, train_labels):\n",
    "            x_train = train_features[train_index]\n",
    "            x_test = train_features[test_index]\n",
    "            y_train = train_labels[train_index]\n",
    "            y_test = train_labels[test_index]\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(x_test).astype(int)\n",
    "            a, p, r, f1 = evaluate(y_test, y_pred)\n",
    "            results['Accuracy'].append(a)\n",
    "            results['Precision'].append(p)\n",
    "            results['Recall'].append(r)\n",
    "            results['F1'].append(f1)\n",
    "\n",
    "        f1_avg = sum(results['F1'])/len(results['F1'])\n",
    "        print(f\"F1 Average (k-fold): {f1_avg}\")\n",
    "        \n",
    "    return results, y_pred\n",
    "\n",
    "submission = False\n",
    "binary_classifier(1, submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf18b3-9cce-404f-87c8-e96698290365",
   "metadata": {},
   "source": [
    "Run the binary classifier for the possible cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "b6aa5eba-ccd6-4258-b125-a1773e301def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff: 1\n",
      "F1 Macro: [0.6940908989590531]\n",
      "\n",
      "\n",
      "Cutoff: 2\n",
      "F1 Macro: [0.7794862271742495]\n",
      "\n",
      "\n",
      "Cutoff: 3\n",
      "F1 Macro: [0.791538140330554]\n",
      "\n",
      "\n",
      "Cutoff: 4\n",
      "F1 Macro: [0.6866609727788364]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(y_pred[0:100])\n",
    "# print(np.count_nonzero(y_pred==0))\n",
    "\n",
    "cutoff = 1\n",
    "max_cutoffs = 4\n",
    "submission = False;\n",
    "\n",
    "if submission == True:\n",
    "    while cutoff <= max_cutoffs:\n",
    "        print(f\"Cutoff: {cutoff}\")\n",
    "        y_pred, y_test = binary_classifier(cutoff, submission)\n",
    "\n",
    "        to_csv(y_pred, \"binary\")\n",
    "        cutoff += 1\n",
    "    \n",
    "elif submission == False:\n",
    "    while cutoff <= max_cutoffs:\n",
    "        print(f\"Cutoff: {cutoff}\")\n",
    "        \n",
    "        y_pred, y_test = binary_classifier(cutoff, submission)\n",
    "        results = evaluate(y_pred, y_test)\n",
    "        f1 = results['F1']\n",
    "            \n",
    "        print(f\"F1 Macro: {f1}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        to_csv(y_pred, \"binary\")\n",
    "        cutoff += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
