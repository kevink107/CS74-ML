{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2f6d42-374d-4998-b7e8-271f29fa856e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CS74 Final Project\n",
    "### Kevin King, Spring 2022\n",
    "### Due: Tuesday, June 7th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16740fdb-5aae-40c9-9e46-7db589345ad4",
   "metadata": {},
   "source": [
    "Import libraries needed for the final project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "d85067e4-01ab-40d9-bfb7-d9f1fe02b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e2b66-8885-4a91-978c-1de0378ad9dc",
   "metadata": {},
   "source": [
    "Import training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "8ded09ce-e3ca-492f-aa6d-03c99a26eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_table('Training.csv', delimiter=',').fillna('NULL')\n",
    "test = pd.read_table('Test.csv', delimiter=',').fillna('NULL')\n",
    "# print(train['overall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f84da2-2a67-4021-ba32-a0f73e814a11",
   "metadata": {},
   "source": [
    "### Binary Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3378f0-66e5-4303-add6-ba2f459e3d95",
   "metadata": {},
   "source": [
    "#### Vectorizing the Features (x values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "aea23ca8-3f26-4029-8cf9-29cec39be66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 3\n",
    "# classes = 1,2,3,4,5\n",
    "max_cutoff = 4\n",
    "\n",
    "train_df = pd.DataFrame(train, columns = ['overall', 'reviewText','category'])\n",
    "x_train = train_df[['reviewText', 'category']].copy()\n",
    "y_train = train_df[['overall']].copy().astype(float)\n",
    "\n",
    "test_df = pd.DataFrame(test, columns = ['overall', 'reviewText','category'])\n",
    "x_test = test_df[['reviewText', 'category']].copy()\n",
    "\n",
    "y_label = y_train.copy()\n",
    "y_label.loc[y_train.overall>cutoff] = 1\n",
    "y_label.loc[y_train.overall<=cutoff] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adcbf0-137b-48a3-ad0e-f5c9a15ac326",
   "metadata": {},
   "source": [
    "#### In order to get numeric features for our text column, we will use Scikit-Learn's TFIDF vectorizer function (from TF-IDF_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "8df4afe5-043a-4d9b-9291-594261de9b0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reviewText'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [435]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Fit vectorizer and transform text into features \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m test_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43my_label\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreviewText\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      8\u001b[0m test_matrix \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_features\u001b[38;5;241m.\u001b[39mtodense())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# comment/uncomment in order to get the table\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/generic.py:5465\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name):\n\u001b[1;32m   5464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reviewText'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initalize vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=0.05, max_df=0.9)\n",
    "\n",
    "# Fit vectorizer and transform text into features \n",
    "test_features = vectorizer.fit_transform(y_label.reviewText.tolist())\n",
    "test_matrix = pd.DataFrame(test_features.todense())\n",
    "\n",
    "# comment/uncomment in order to get the table\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd332f-0d4b-4668-8e86-59fce6415d38",
   "metadata": {},
   "source": [
    "#### Sort the Labels into Binary Classes (y values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "c315bfab-36ce-49bd-a66a-2cc7300dff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4500, 29189]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [429]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression \n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_matrix)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_predict)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1508\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1506\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1508\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1516\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    964\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    965\u001b[0m     X,\n\u001b[1;32m    966\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    976\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    977\u001b[0m )\n\u001b[1;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m--> 981\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    335\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4500, 29189]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(test_matrix, y_label)\n",
    "y_predict = model.predict(test_matrix)\n",
    "print(y_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d663011-e359-41e2-85c1-fbcecda381ce",
   "metadata": {},
   "source": [
    "#### Test features in the test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "5e39a5ad-22ff-49d5-818b-206c0fd0b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              reviewText    category\n",
      "0          all of the reviews for this product are fake.  automotive\n",
      "1                                 wrong part. our fault.  automotive\n",
      "2                       this wire set it really sucks!!!  automotive\n",
      "3      first use, it leaked instantly. even at 5 buck...  automotive\n",
      "4                                             didn't fit  automotive\n",
      "...                                                  ...         ...\n",
      "29184  this is the same plush toy that the official d...        toys\n",
      "29185  my grandson loved this. it is a great toy, he ...        toys\n",
      "29186  my kiddo loves them! we are a rock climbing fa...        toys\n",
      "29187  i bought this for my niece (age 2) and mailed ...        toys\n",
      "29188  my daughter will love this! she's a huge ninja...        toys\n",
      "\n",
      "[29189 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [431]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m y_train_labeled\u001b[38;5;241m.\u001b[39mloc[y_train\u001b[38;5;241m.\u001b[39moverall \u001b[38;5;241m>\u001b[39m cutoff] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m y_train_labeled\u001b[38;5;241m.\u001b[39mloc[y_train\u001b[38;5;241m.\u001b[39moverall \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m cutoff] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m train_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract the feature names (for visualization purposes)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m extracted_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/generic.py:5465\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name):\n\u001b[1;32m   5464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "\n",
    "# select features of the test file\n",
    "x_test = pd.DataFrame([test['reviewText'], test['category']]).T\n",
    "x_test.columns = ['text', 'category']\n",
    "print(x_train)\n",
    "\n",
    "y_train_labeled = y_train.copy()\n",
    "y_train_labeled.loc[y_train.overall > cutoff] = 1\n",
    "y_train_labeled.loc[y_train.overall <= cutoff] = 0\n",
    "\n",
    "\n",
    "train_features = vectorizer.fit_transform(x_train.text.tolist())\n",
    "\n",
    "# Extract the feature names (for visualization purposes)\n",
    "extracted_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dataframe showing what the TF-IDF feature values are for each word in each samples\n",
    "review_text_dense_matrix = pd.DataFrame(train_features.todense())\n",
    "review_text_dense_matrix.columns = extracted_features\n",
    "review_text_dense_matrix.index = ['sample_'+str(x) for x in review_text_dense_matrix.index]\n",
    "\n",
    "# Fit vectorizer and transform text into features \n",
    "test_features = vectorizer.transform(x_test.text.tolist())\n",
    "test_dense_matrix = pd.DataFrame(test_features.todense())\n",
    "\n",
    "test_dense_matrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features, x_train_labeled.overall.tolist())\n",
    "print(model.predict(test_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0b8e2-4440-4a0c-996c-112f166b2bd7",
   "metadata": {},
   "source": [
    "#### Test random line from test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6fa24e-a34d-4d32-b706-02caef831c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random line from the test file\n",
    "num_lines = test[test.columns[0]].count()\n",
    "rand_line = np.random.randint(0, num_lines)\n",
    "test_sample = [test['reviewText'][rand_line]]\n",
    "# print(f\"Test Sample: {test_sample}\")\n",
    "\n",
    "# Transform into TF-IDF representation using vectorizers we fit earlier \n",
    "test_feature = vectorizer.transform(test_sample)\n",
    "# Create visualization of output matrix\n",
    "dense_matrix = pd.DataFrame(test_feature.todense())\n",
    "dense_matrix.columns = extracted_features\n",
    "dense_matrix.index = ['test_sample']\n",
    "\n",
    "# dense_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "74d7e223-39b9-4b0c-baad-f0fe222eb703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      overall                                               text    category\n",
      "0           1      all of the reviews for this product are fake.  automotive\n",
      "1           1                             wrong part. our fault.  automotive\n",
      "2           1                   this wire set it really sucks!!!  automotive\n",
      "3           1  first use, it leaked instantly. even at 5 buck...  automotive\n",
      "4           1                                         didn't fit  automotive\n",
      "...       ...                                                ...         ...\n",
      "29184       5  this is the same plush toy that the official d...        toys\n",
      "29185       5  my grandson loved this. it is a great toy, he ...        toys\n",
      "29186       5  my kiddo loves them! we are a rock climbing fa...        toys\n",
      "29187       5  i bought this for my niece (age 2) and mailed ...        toys\n",
      "29188       5  my daughter will love this! she's a huge ninja...        toys\n",
      "\n",
      "[29189 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# draft code\n",
    "\n",
    "# train_import = np.genfromtxt('Training.csv', delimiter=',', dtype=None, invalid_raise=False, filling_values='',\n",
    "#                    names=('overall','verified','reviewTime','reviewerID','asin','reviewerName',\n",
    "#                          'reviewText','summary','unixReviewTime','vote','image','style','category'))\n",
    "\n",
    "# test_import = np.genfromtxt('Test.csv', delimiter=',', dtype=None, invalid_raise=False, filling_values='',\n",
    "#                    names=('verified','reviewTime','reviewerID','asin','reviewerName',\n",
    "#                          'reviewText','summary','unixReviewTime','vote','image','style','category'))\n",
    "\n",
    "# y_train = all the classes of the train file\n",
    "y_train = pd.DataFrame([train['overall']]).T\n",
    "y_train.columns = ['overall']\n",
    "\n",
    "# lines in train data below or equal to the cutoff\n",
    "good = train[train['overall'] <= cutoff]\n",
    "\n",
    "# lines in train data above the cutoff\n",
    "bad = train[train['overall']> cutoff] \n",
    "\n",
    "# features of the train data below or equal to the cutoff\n",
    "x_train = pd.DataFrame([train['overall'], train['reviewText'], train['category']]).T\n",
    "x_train.columns = ['overall', 'text', 'category']\n",
    "print(x_train)\n",
    "\n",
    "# for item in train['category']:\n",
    "#     print(item)\n",
    "# separates by above and below cutoff\n",
    "\n",
    "# good_category = pd.DataFrame([good['overall'], good['category']]).T\n",
    "# good_category.columns = ['overall', 'category']\n",
    "\n",
    "# good_reviewText = pd.DataFrame([good['overall'], good['reviewText']]).T\n",
    "# good_reviewText.columns = ['overall', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6dc235-ed0a-4b43-8d3f-bc824f6b8553",
   "metadata": {},
   "source": [
    "### Other Draft Code - transferred from \"CS74_final.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c47e9d-2169-489f-a08e-22ae8ddb58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(cutoff, y_label):\n",
    "    # Hyperparameter Tuning\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_features, y_label, test_size=0.2)\n",
    "    \n",
    "    c_list = list(range(0,10,1))\n",
    "                                                       \n",
    "    params = {\n",
    "        'penalty': ['l2'],\n",
    "        'C': c_list,\n",
    "        'solver': ['lbfgs','liblinear'],\n",
    "    }\n",
    "\n",
    "    logreg = LogisticRegression(max_iter=100)\n",
    "    \n",
    "    clf = GridSearchCV(logreg,                # model\n",
    "                       param_grid = params,   # hyperparameters\n",
    "                       scoring='f1_macro',        # metric for scoring\n",
    "                       cv=6, refit=True) \n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def print_and_csv_ht(cutoff, clf):\n",
    "    print(\"Tuned Hyperparameters :\", clf.best_params_)\n",
    "    print(\"F1 Macro :\",clf.best_score_)\n",
    "    print(clf.best_estimator_)\n",
    "    \n",
    "    prediction = clf.predict(test_features).astype(int)\n",
    "    out = {'id': range(len(prediction)), 'predicted': prediction}\n",
    "    outdf = pd.DataFrame(out)\n",
    "    outdf.to_csv(f'{cutoff}.csv', index=False)\n",
    "\n",
    "def binary_classifier(cutoff, submission):\n",
    "    # labels classes \n",
    "    y_label = construct_labels(cutoff)\n",
    "    \n",
    "    # model\n",
    "    model = LogisticRegression(fit_intercept=False, solver='saga') # change max_iter\n",
    "    \n",
    "    # results dictionary\n",
    "    results = {'Accuracy' : [],\n",
    "               'Precision' : [],\n",
    "               'Recall' : [],\n",
    "               'F1' : []}\n",
    "    \n",
    "    # model prediction\n",
    "    if submission == True:\n",
    "        model.fit(train_features, y_label)\n",
    "        y_pred = model.predict(test_features).astype(int) # use this when submitting to Kaggle\n",
    "    \n",
    "    elif submission == False:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(train_features, y_label, test_size=0.1)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test).astype(int)\n",
    "    \n",
    "        a, p, r, f1 = evaluate(y_test, y_pred)\n",
    "        results['Accuracy'].append(a)\n",
    "        results['Precision'].append(p)\n",
    "        results['Recall'].append(r)\n",
    "        results['F1'].append(f1)\n",
    "        \n",
    "    return results, y_pred\n",
    "    \n",
    "# print(y_pred[0:100])\n",
    "# print(np.count_nonzero(y_pred==0))\n",
    "\n",
    "cutoff = 1\n",
    "max_cutoffs = 4\n",
    "submission = False;\n",
    "\n",
    "if submission == True:\n",
    "    while cutoff <= max_cutoffs:\n",
    "        print(f\"Cutoff: {cutoff}\")\n",
    "        results, y_pred = binary_classifier(cutoff, submission)\n",
    "        make_csv(y_pred, \"binary\", cutoff)\n",
    "        cutoff += 1\n",
    "    \n",
    "elif submission == False:\n",
    "    while cutoff <= max_cutoffs:\n",
    "        print(f\"Cutoff: {cutoff}\")\n",
    "        results, y_pred = binary_classifier(cutoff, submission)\n",
    "        f1 = results['F1']\n",
    "        print(f\"F1 Macro: {f1}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        make_csv(y_pred, \"binary\", cutoff)\n",
    "        cutoff += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
