{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa52025f",
   "metadata": {},
   "source": [
    "# CS74 Final Project\n",
    "### Kevin King, Spring 2022\n",
    "### Due: Tuesday, June 7th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e82930",
   "metadata": {},
   "source": [
    "Import general libraries needed for the final project (more to be imported later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414b8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746b18b",
   "metadata": {},
   "source": [
    "### Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a437ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = pd.read_table('Training.csv', delimiter=',').fillna('NULL')\n",
    "test_table = pd.read_table('Test.csv', delimiter=',').fillna('NULL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4164c4",
   "metadata": {},
   "source": [
    "## Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0efa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data: y = classes, x = features\n",
    "train_df = pd.DataFrame(train_table, columns = ['overall', 'reviewText','category', 'summary', 'verified', 'unixReviewTime', 'asin'])\n",
    "y_train_df = train_df[['overall']].copy().astype(float)\n",
    "x_train_df = train_df[['reviewText', 'category', 'summary', 'verified', 'unixReviewTime', 'asin']].copy()\n",
    "\n",
    "# test data: x = features\n",
    "test_df = pd.DataFrame(test_table, columns = ['overall', 'reviewText','category', 'summary', 'verified', 'unixReviewTime', 'asin'])\n",
    "x_test_df = test_df[['reviewText', 'category', 'summary', 'verified', 'unixReviewTime', 'asin']].copy()\n",
    "\n",
    "# vectorize train and test features - 2 different ones to improve accuracy\n",
    "# vectorizer for reviewText\n",
    "vectorizer_rt = TfidfVectorizer()\n",
    "train_rt = vectorizer_rt.fit_transform(x_train_df.reviewText.tolist())\n",
    "test_rt = vectorizer_rt.transform(x_test_df.reviewText.tolist())\n",
    "\n",
    "#vectorizer for summary\n",
    "vectorizer_sum = TfidfVectorizer()\n",
    "train_sum = vectorizer_sum.fit_transform(x_train_df.summary.tolist())\n",
    "test_sum = vectorizer_sum.transform(x_test_df.summary.tolist())\n",
    "\n",
    "# train features, test features\n",
    "train_features = hstack((train_rt, train_sum)).tocsr()\n",
    "test_features = hstack((test_rt, test_sum)).tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f2517",
   "metadata": {},
   "source": [
    "#### Setup and Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c305227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# prints the predictions to a csv file for Kaggle\n",
    "def make_csv(model_method, y_pred, type_name, num):\n",
    "    out = {'id': range(len(y_pred)), 'predicted': y_pred}\n",
    "    outdf = pd.DataFrame(out)\n",
    "    outdf.to_csv(f'{type_name}_{model_method}_{num}.csv', index=False)\n",
    "\n",
    "# creates labels based on cutoff\n",
    "def construct_labels(train_df, cutoff):\n",
    "    train_label = train_df.overall.copy()\n",
    "    train_label.loc[train_df.overall > cutoff] = 1\n",
    "    train_label.loc[train_df.overall <= cutoff] = 0\n",
    "    return train_label\n",
    "\n",
    "# gets accuracy, precision, etc. and adds to results dictionary\n",
    "def evaluate(results, y_labels, y_pred):\n",
    "    # get values for accuracy, precision, recall, f1\n",
    "    assert len(y_labels) == len(y_pred), 'labels array and predictions array must be the same length'\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_labels, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_labels, y_pred)\n",
    "    roc_auc = roc_auc_score(y_labels, y_pred)\n",
    "    c_matrix = confusion_matrix(y_labels, y_pred)\n",
    "    \n",
    "    # append values to results dictionary\n",
    "    results['Accuracy'].append(accuracy)\n",
    "    results['Precision'].append(precision)\n",
    "    results['Recall'].append(recall)\n",
    "    results['F1'].append(f1)\n",
    "    results['ROC_AUC'].append(roc_auc)\n",
    "    results['Confusion Matrix'].append(c_matrix)\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb150a",
   "metadata": {},
   "source": [
    "#### Binary Classifier Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d70f984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classifier(model, cutoff, submission):  \n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    # results dictionary\n",
    "    results = {'Accuracy' : [],\n",
    "               'Precision' : [],\n",
    "               'Recall' : [],\n",
    "               'F1' : [],\n",
    "               'ROC_AUC': [],\n",
    "               'Confusion Matrix': []}\n",
    "    \n",
    "    train_labels = construct_labels(y_train_df, cutoff)\n",
    "    matrix = np.zeros((2,2))\n",
    "    \n",
    "    # to submit to Kaggle\n",
    "    if submission == True:\n",
    "        model.fit(train_features, train_labels)\n",
    "        y_pred = model.predict(test_features).astype(int)\n",
    "        print(f\"File {cutoff} ready for submission\\n\")\n",
    "    \n",
    "    # not for Kaggle - testing purposes\n",
    "    elif submission == False:\n",
    "        # uses k_folds to get F1 average\n",
    "        for train_index, test_index in kf.split(train_features, train_labels):\n",
    "            x_train = train_features[train_index]\n",
    "            x_test = train_features[test_index]\n",
    "            y_train = train_labels[train_index]\n",
    "            y_test = train_labels[test_index]\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred = model.predict(x_test).astype(int)\n",
    "\n",
    "            results = evaluate(results, y_test, y_pred)\n",
    "\n",
    "        # compute output results\n",
    "        f1_avg = sum(results['F1']) / len(results['F1'])\n",
    "        accuracy_avg = sum(results['Accuracy']) / len(results['Accuracy'])\n",
    "        precision_avg = sum(results['Precision']) / len(results['Precision'])\n",
    "        roc_auc_avg = sum(results['ROC_AUC']) / len(results['ROC_AUC'])\n",
    "        \n",
    "        # confusion matrix\n",
    "        matrix = np.zeros((2,2))\n",
    "        for item in results['Confusion Matrix']:\n",
    "            matrix = np.add(matrix, item)\n",
    "        matrix_avg = matrix / len(results['Confusion Matrix'])\n",
    "\n",
    "        # print out results\n",
    "        print(f\"Accuracy Average:{accuracy_avg}\")\n",
    "        print(f\"Precision Average: {precision_avg}\")\n",
    "        print(f\"F1 Score Average: {f1_avg}\")\n",
    "        print(f\"ROC AUC Score Average: {roc_auc_avg}\")\n",
    "        print(f\"Confusion Matrix Average:\\n {matrix_avg[0]}\\n {matrix_avg[1]}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d044f",
   "metadata": {},
   "source": [
    "#### Import SK-Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e89e5994-02db-4e80-b9c2-9efa808811cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd8e17d-cb2c-4267-9f36-d788be285348",
   "metadata": {},
   "source": [
    "#### Cutoff 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9d90053e-51c4-4914-9d70-cf6ee3ef26ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY CLASSIFIER - CUTOFF 1\n",
      "\n",
      "Logistic Regression Model: \n",
      "\n",
      "Accuracy Average:0.7908153870452184\n",
      "Precision Average: 0.7559915567622599\n",
      "F1 Score Average: 0.6799931086307246\n",
      "ROC AUC Score Average: 0.6595720993613837\n",
      "Confusion Matrix Average:\n",
      " [483.8 707.6]\n",
      " [ 513.6 4132.8]\n",
      "\n",
      "\n",
      "Perceptron Model: \n",
      "\n",
      "Accuracy Average:0.7462094858242974\n",
      "Precision Average: 0.6717359736701012\n",
      "F1 Score Average: 0.6501400701552121\n",
      "ROC AUC Score Average: 0.6569316921524655\n",
      "Confusion Matrix Average:\n",
      " [570.4 621. ]\n",
      " [ 860.6 3785.8]\n",
      "\n",
      "\n",
      "Linear SVC Model: \n",
      "\n",
      "Accuracy Average:0.7674507575710889\n",
      "Precision Average: 0.7126321087861431\n",
      "F1 Score Average: 0.670847304058305\n",
      "ROC AUC Score Average: 0.6644959452385926\n",
      "Confusion Matrix Average:\n",
      " [549.  642.4]\n",
      " [ 715.2 3931.2]\n",
      "\n",
      "\n",
      "Best Model: \n",
      "\n",
      "Accuracy Average:0.7849914571389952\n",
      "Precision Average: 0.7406847617912109\n",
      "F1 Score Average: 0.6839033625538858\n",
      "ROC AUC Score Average: 0.6695940162488385\n",
      "Confusion Matrix Average:\n",
      " [528.2 663.2]\n",
      " [ 592.  4054.4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# switch to True for Kaggle submission mode\n",
    "submission = False\n",
    "cutoff = 1\n",
    "\n",
    "print(f\"BINARY CLASSIFIER - CUTOFF {cutoff}\\n\")\n",
    "\n",
    "print(f\"Logistic Regression Model: \\n\")\n",
    "model = LogisticRegression()\n",
    "model_name = \"Logistic Regression\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Perceptron Model: \\n\")\n",
    "model = Perceptron()\n",
    "model_name = \"Perceptron\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Linear SVC Model: \\n\")\n",
    "model = LinearSVC()\n",
    "model_name = \"SVC\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Best Model: \\n\")\n",
    "model = LogisticRegression(fit_intercept = False)\n",
    "model_name = \"Best\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c98011-e7b0-4a02-a33d-94b06eb878cd",
   "metadata": {},
   "source": [
    "#### Cutoff 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "13eac066-9b2f-436e-b91c-66b1be1f9b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY CLASSIFIER - CUTOFF 2\n",
      "\n",
      "Logistic Regression Model: \n",
      "\n",
      "Accuracy Average:0.7330550821586056\n",
      "Precision Average: 0.7259076373737119\n",
      "F1 Score Average: 0.7230177962191434\n",
      "ROC AUC Score Average: 0.727084514926615\n",
      "Confusion Matrix Average:\n",
      " [1678.6  704.6]\n",
      " [ 853.8 2600.8]\n",
      "\n",
      "\n",
      "Perceptron Model: \n",
      "\n",
      "Accuracy Average:0.6898193019533809\n",
      "Precision Average: 0.6803327140171975\n",
      "F1 Score Average: 0.6777505273651421\n",
      "ROC AUC Score Average: 0.6842525121166843\n",
      "Confusion Matrix Average:\n",
      " [1614.6  768.6]\n",
      " [1042.2 2412.4]\n",
      "\n",
      "\n",
      "Linear SVC Model: \n",
      "\n",
      "Accuracy Average:0.717638315496065\n",
      "Precision Average: 0.709694186987321\n",
      "F1 Score Average: 0.7070989557045472\n",
      "ROC AUC Score Average: 0.7125236334364298\n",
      "Confusion Matrix Average:\n",
      " [1681.   702.2]\n",
      " [ 946.2 2508.4]\n",
      "\n",
      "\n",
      "Best Model: \n",
      "\n",
      "Accuracy Average:0.7374058402755267\n",
      "Precision Average: 0.7293974312670842\n",
      "F1 Score Average: 0.7271298528296007\n",
      "ROC AUC Score Average: 0.7308487870780336\n",
      "Confusion Matrix Average:\n",
      " [1674.4  708.8]\n",
      " [ 824.2 2630.4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# switch to True for Kaggle submission mode\n",
    "submission = False\n",
    "cutoff = 2\n",
    "\n",
    "print(f\"BINARY CLASSIFIER - CUTOFF {cutoff}\\n\")\n",
    "\n",
    "print(f\"Logistic Regression Model: \\n\")\n",
    "model = LogisticRegression()\n",
    "model_name = \"Logistic Regression\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Perceptron Model: \\n\")\n",
    "model = Perceptron()\n",
    "model_name = \"Perceptron\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Linear SVC Model: \\n\")\n",
    "model = LinearSVC()\n",
    "model_name = \"SVC\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Best Model: \\n\")\n",
    "model = LogisticRegression(fit_intercept = False, C=0.4, max_iter=1000)\n",
    "model_name = \"Best\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594154a5-dfa8-473d-8a06-41fc44ff660c",
   "metadata": {},
   "source": [
    "#### Cutoff 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fc9cd114-0fc5-4f60-905d-8a6cda2c0cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY CLASSIFIER - CUTOFF 3\n",
      "\n",
      "Logistic Regression Model: \n",
      "\n",
      "Accuracy Average:0.7884504604153385\n",
      "Precision Average: 0.7865316862097389\n",
      "F1 Score Average: 0.7538160045349208\n",
      "ROC AUC Score Average: 0.7510590028827466\n",
      "Confusion Matrix Average:\n",
      " [3208.2  347.4]\n",
      " [ 887.6 1394.6]\n",
      "\n",
      "\n",
      "Perceptron Model: \n",
      "\n",
      "Accuracy Average:0.744632582438418\n",
      "Precision Average: 0.7257967623158794\n",
      "F1 Score Average: 0.7150280097210108\n",
      "ROC AUC Score Average: 0.7155361727849183\n",
      "Confusion Matrix Average:\n",
      " [2954.6  601. ]\n",
      " [ 889.8 1392.4]\n",
      "\n",
      "\n",
      "Linear SVC Model: \n",
      "\n",
      "Accuracy Average:0.7712181853919688\n",
      "Precision Average: 0.7579095943570767\n",
      "F1 Score Average: 0.7408103431285216\n",
      "ROC AUC Score Average: 0.7402185125224704\n",
      "Confusion Matrix Average:\n",
      " [3072.8  482.8]\n",
      " [ 852.8 1429.4]\n",
      "\n",
      "\n",
      "Best Model: \n",
      "\n",
      "Accuracy Average:0.7885875758141865\n",
      "Precision Average: 0.7822326065209326\n",
      "F1 Score Average: 0.7569151314346408\n",
      "ROC AUC Score Average: 0.7551918840997422\n",
      "Confusion Matrix Average:\n",
      " [3167.   388.6]\n",
      " [ 845.6 1436.6]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# switch to True for Kaggle submission mode\n",
    "submission = False\n",
    "cutoff = 3\n",
    "\n",
    "print(f\"BINARY CLASSIFIER - CUTOFF {cutoff}\\n\")\n",
    "\n",
    "print(f\"Logistic Regression Model: \\n\")\n",
    "model = LogisticRegression()\n",
    "model_name = \"Logistic Regression\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Perceptron Model: \\n\")\n",
    "model = Perceptron()\n",
    "model_name = \"Perceptron\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Linear SVC Model: \\n\")\n",
    "model = LinearSVC()\n",
    "model_name = \"SVC\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Best Model: \\n\")\n",
    "model = LogisticRegression(fit_intercept = False, max_iter=100, solver='saga')\n",
    "model_name = \"Best\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d23555-4b3d-4cf5-87ce-3a5e7053fa01",
   "metadata": {},
   "source": [
    "#### Cutoff 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5c488036-5df4-4d58-a00b-bcc147bbcbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY CLASSIFIER - CUTOFF 4\n",
      "\n",
      "Logistic Regression Model: \n",
      "\n",
      "Accuracy Average:0.8741284805680506\n",
      "Precision Average: 0.8588986918360751\n",
      "F1 Score Average: 0.7429660240813721\n",
      "ROC AUC Score Average: 0.7075559835684679\n",
      "Confusion Matrix Average:\n",
      " [4612.4   97. ]\n",
      " [637.8 490.6]\n",
      "\n",
      "\n",
      "Perceptron Model: \n",
      "\n",
      "Accuracy Average:0.846310658465567\n",
      "Precision Average: 0.7574091949902111\n",
      "F1 Score Average: 0.7271409683389495\n",
      "ROC AUC Score Average: 0.7191343998626767\n",
      "Confusion Matrix Average:\n",
      " [4369.   340.4]\n",
      " [556.8 571.6]\n",
      "\n",
      "\n",
      "Linear SVC Model: \n",
      "\n",
      "Accuracy Average:0.8652216433857489\n",
      "Precision Average: 0.8074012068999933\n",
      "F1 Score Average: 0.7450814748750274\n",
      "ROC AUC Score Average: 0.7219175201320026\n",
      "Confusion Matrix Average:\n",
      " [4501.6  207.8]\n",
      " [579.  549.4]\n",
      "\n",
      "\n",
      "Best Model: \n",
      "\n",
      "Accuracy Average:0.8770410412412624\n",
      "Precision Average: 0.8401897055976473\n",
      "F1 Score Average: 0.7580952429093939\n",
      "ROC AUC Score Average: 0.726447576079344\n",
      "Confusion Matrix Average:\n",
      " [4572.   137.4]\n",
      " [580.4 548. ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# switch to True for Kaggle submission mode\n",
    "submission = False\n",
    "cutoff = 4\n",
    "\n",
    "print(f\"BINARY CLASSIFIER - CUTOFF {cutoff}\\n\")\n",
    "\n",
    "print(f\"Logistic Regression Model: \\n\")\n",
    "model = LogisticRegression()\n",
    "model_name = \"Logistic Regression\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Perceptron Model: \\n\")\n",
    "model = Perceptron()\n",
    "model_name = \"Perceptron\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Linear SVC Model: \\n\")\n",
    "model = LinearSVC()\n",
    "model_name = \"SVC\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)\n",
    "\n",
    "print(f\"Best Model: \\n\")\n",
    "model = LogisticRegression(fit_intercept = False)\n",
    "model_name = \"Best\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"binary\", cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528d63e",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "01e9878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass(model_method, submission):\n",
    "    kf = KFold(n_splits=5) # can change\n",
    "    \n",
    "    train_labels = train_df[['overall']].copy().astype(int)\n",
    "    \n",
    "    # Kaggle submission mode\n",
    "    if submission == True:\n",
    "        model.fit(train_features, train_labels)\n",
    "        y_pred = model.predict(test_features).astype(int)\n",
    "        print(f\"File is ready for submission\")\n",
    "        \n",
    "    elif submission == False:\n",
    "        # uses k_folds to get F1 average\n",
    "        for train_index, test_index in kf.split(train_features, train_labels):\n",
    "            x_train = train_features[train_index]\n",
    "            x_test = train_features[test_index]\n",
    "            y_train = train_labels[train_index]\n",
    "            y_test = train_labels[test_index]\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred = model.predict(x_test).astype(int)\n",
    "\n",
    "            results = evaluate(results, y_test, y_pred)\n",
    "\n",
    "        # compute output results\n",
    "        f1_avg = sum(results['F1']) / len(results['F1'])\n",
    "        accuracy_avg = sum(results['Accuracy']) / len(results['Accuracy'])\n",
    "        precision_avg = sum(results['Precision']) / len(results['Precision'])\n",
    "        roc_auc_avg = sum(results['ROC_AUC']) / len(results['ROC_AUC'])\n",
    "        \n",
    "        # confusion matrix\n",
    "        matrix = np.zeros((2,2))\n",
    "        for item in results['Confusion Matrix']:\n",
    "            matrix = np.add(matrix, item)\n",
    "        matrix_avg = matrix / len(results['Confusion Matrix'])\n",
    "        \n",
    "        # print outputs\n",
    "        print(f\"Accuracy Average:{accuracy_avg}\")\n",
    "        print(f\"Precision Average: {precision_avg}\")\n",
    "        print(f\"F1 Score Average: {f1_avg}\")\n",
    "        print(f\"ROC AUC Score Average: {roc_auc_avg}\")\n",
    "        print(f\"Confusion Matrix Average:\\n {matrix_avg[0]}\\n {matrix_avg[1]}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72506793-1c40-471e-8195-d0f479a953ac",
   "metadata": {},
   "source": [
    "#### Run the Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a3f2013c-6169-45eb-9e0a-65527145615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTICLASS\n",
      "\n",
      "Logistic Regression Model: \n",
      "\n",
      "Accuracy Average:0.7884504604153385\n",
      "Precision Average: 0.7865316862097389\n",
      "F1 Score Average: 0.7538160045349208\n",
      "ROC AUC Score Average: 0.7510590028827466\n",
      "Confusion Matrix Average:\n",
      " [3208.2  347.4]\n",
      " [ 887.6 1394.6]\n",
      "\n",
      "\n",
      "Perceptron Model: \n",
      "\n",
      "Accuracy Average:0.744632582438418\n",
      "Precision Average: 0.7257967623158794\n",
      "F1 Score Average: 0.7150280097210108\n",
      "ROC AUC Score Average: 0.7155361727849183\n",
      "Confusion Matrix Average:\n",
      " [2954.6  601. ]\n",
      " [ 889.8 1392.4]\n",
      "\n",
      "\n",
      "Linear SVC Model: \n",
      "\n",
      "Accuracy Average:0.7712181853919688\n",
      "Precision Average: 0.7579095943570767\n",
      "F1 Score Average: 0.7408103431285216\n",
      "ROC AUC Score Average: 0.7402185125224704\n",
      "Confusion Matrix Average:\n",
      " [3072.8  482.8]\n",
      " [ 852.8 1429.4]\n",
      "\n",
      "\n",
      "Best Model: \n",
      "\n",
      "Accuracy Average:0.7882111746174171\n",
      "Precision Average: 0.7737577867577881\n",
      "F1 Score Average: 0.7629052167138334\n",
      "ROC AUC Score Average: 0.7636954899147825\n",
      "Confusion Matrix Average:\n",
      " [3035.   520.6]\n",
      " [ 715.8 1566.4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# switch to True for Kaggle submission mode\n",
    "submission = False;\n",
    "\n",
    "# different types of models\n",
    "\n",
    "print(f\"MULTICLASS\\n\")\n",
    "\n",
    "print(f\"Logistic Regression Model: \\n\")\n",
    "model = LogisticRegression()\n",
    "model_name = \"Logistic Regression\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"multiclass\", cutoff)\n",
    "\n",
    "print(f\"Perceptron Model: \\n\")\n",
    "model = Perceptron()\n",
    "model_name = \"Perceptron\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"multiclass\", cutoff)\n",
    "\n",
    "print(f\"Linear SVC Model: \\n\")\n",
    "model = LinearSVC()\n",
    "model_name = \"SVC\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"multiclass\", cutoff)\n",
    "\n",
    "print(f\"Best Model: \\n\")\n",
    "model = LogisticRegression(fit_intercept=False, multi_class='ovr', class_weight='balanced')\n",
    "model_name = \"Best\"\n",
    "predictions = binary_classifier(model, cutoff, submission)\n",
    "make_csv(model_name, predictions, \"multiclass\", cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d5efe",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab261d-5fca-468e-9154-e681e12fa5d7",
   "metadata": {},
   "source": [
    "#### New Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d43fc06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# new test data\n",
    "test_df = pd.DataFrame(test_table, columns = ['reviewText','category', 'summary'])\n",
    "y_test_df = test_df[['category']].copy()\n",
    "x_test_df = test_df[['reviewText','summary']].copy()\n",
    "\n",
    "# label each category with numbers\n",
    "labels = y_test_df.category.unique()\n",
    "num_labels = len(labels)\n",
    "label_indices = list(range(0, num_labels))\n",
    "y_test_df.category.replace(labels, label_indices, inplace=True)\n",
    "true_labels = np.array(y_test_df).flatten()\n",
    "print(true_labels)\n",
    "\n",
    "# vectorize features - reviewText, summary\n",
    "vectorizer_rt = TfidfVectorizer(stop_words='english', min_df=0.2, max_df=0.9)\n",
    "test_rt = vectorizer_rt.fit_transform(x_test_df.reviewText.tolist())\n",
    "\n",
    "vectorizer_sum = TfidfVectorizer(stop_words='english', min_df=0.1, max_df=0.9)\n",
    "test_sum = vectorizer_sum.fit_transform(x_test_df.summary.tolist())\n",
    "\n",
    "test_features = hstack((test_rt, test_sum)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13914917-fa77-436b-9825-3670e0bf4fb4",
   "metadata": {},
   "source": [
    "#### Clustering Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b2c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import rand_score\n",
    "\n",
    "def clustering(num_clusters):\n",
    "    # vectorizes train and test features - words to numbers\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(test_features)\n",
    "    s_score = silhouette_score(test_features, kmeans.labels_)\n",
    "    r_score = rand_score(true_labels, kmeans.labels_)\n",
    "    \n",
    "    return s_score, r_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc04501-c168-4744-a1d2-b74fcbd9a6bb",
   "metadata": {},
   "source": [
    "#### Run the Clustering Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce4b430b-7147-428a-bc9d-5f7cb5db467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTERING\n",
      "Silhouette Score: 0.9554411607742243\n",
      "Random Score: 0.6250878466819787\n"
     ]
    }
   ],
   "source": [
    "num_clusters = num_labels\n",
    "s, r = clustering(num_clusters)\n",
    "\n",
    "print(\"CLUSTERING\")\n",
    "print(f\"Silhouette Score: {s}\")\n",
    "print(f\"Random Score: {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a54744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
