{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS74 HW4\n",
    "## Kevin King\n",
    "## Due Monday, May 30, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given training dataset in CSV format: hw4_naive.csv\n",
    "Training data has 5,600 rows:\n",
    "* Columns 1 through 6 of the CSV file represent the features (X)\n",
    "* The last column (\"Label\") represents the class label (Y) (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below to import libraries needed for this HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) [5 points] Divide the data into test / train sets (80% and 20% respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hw4_naive.csv data\n",
    "data = np.loadtxt('hw4_naive.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "# parse data into x and y\n",
    "x = data[:,:6] # x represents the features\n",
    "y = data[:,6:] # y represents the class label (0 or 1)\n",
    "\n",
    "# split data into test/train stuff\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2) # test_size?\n",
    "\n",
    "# print(f\"x_train: \\n {x_train} \\n length: {len(x_train)}\\n\")\n",
    "# print(f\"x_test: \\n {x_test} \\n length: {len(x_test)}\\n\")\n",
    "# print(f\"y_train: \\n {y_train} \\n length: {len(y_train)}\\n\")\n",
    "# print(f\"y_test: \\n {y_test} \\n length: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) [25 points] Implement a Multinomial Naïve Bayes classifier from scratch, with smoothing (you can set the default smoothing value to 1). You are free to code this up however you like, however, make sure that there is a function that can be called with a test X vector and returns the predicted Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x represents the features\n",
    "# y represents the class label (0 or 1)\n",
    "\n",
    "def multinomial_nb_classifier(x_test, x_train, y_train, classes):\n",
    "    y_pred = np.zeros((1, len(x_test))) # initialize predicted class\n",
    "    s = 1 # default smoothing value\n",
    "    num_classes = len(classes) # number of classes\n",
    "    \n",
    "    # predicting the class of the first row using other rows\n",
    "    for i in range(len(x_test)):\n",
    "        prob = np.zeros(num_classes)\n",
    "        \n",
    "        # probability of each class given the x'\n",
    "        for c in range(num_classes):\n",
    "            curr_class = classes[c]\n",
    "            y_class = np.where(y_train == curr_class)\n",
    "            x_class = x_train[y_class,:][0] # indices in x_train where y_train = class\n",
    "            class_count = len(y_class[0])\n",
    "            \n",
    "            # probability of each feature x given the class\n",
    "            # x_test[i] = class, x_test[i][x] = feature in the class, x_class[:,x] = xth column of x_class\n",
    "            for x in range(len(x_test[i])):\n",
    "                # where x_class and x_test features match\n",
    "                feature_count = np.count_nonzero(x_class[:,x] == x_test[i][x])\n",
    "                \n",
    "                # laplace smoothing\n",
    "                prob[c] += np.log((feature_count + s) / (class_count + num_classes))\n",
    "            \n",
    "            # probability of the class\n",
    "            prob[c] += np.log((class_count + num_classes / len(y_train)))\n",
    "            \n",
    "        y_pred[0][i] = np.argmax(prob)\n",
    "        \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing link: https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) [25 points] Implement a Gaussian Naïve Bayes classier from scratch (no need for smoothing here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nb_classifier(x_test, x_train, y_train, classes):\n",
    "    y_pred = np.zeros((1, len(x_test)))\n",
    "    num_classes = len(classes) # number of classes\n",
    "    \n",
    "    # predicting the class of the first row using other rows\n",
    "    for i in range(len(x_test)):\n",
    "        prob = np.zeros(num_classes)\n",
    "        \n",
    "        # probability of each class given the x'\n",
    "        for c in range(num_classes):\n",
    "            curr_class = classes[c]\n",
    "            y_class = np.where(y_train == curr_class)\n",
    "            x_class = x_train[y_class,:][0] # indices in x_train where y_train = class\n",
    "            class_count = len(y_class[0])\n",
    "            \n",
    "            # probability of each feature x given the class\n",
    "            for x in range(len(x_test[i])):\n",
    "                # mean and standard deviation of the xth column of each x_class\n",
    "                mean = np.mean(x_class[:,x]) \n",
    "                std = np.std(x_class[:,x])\n",
    "                \n",
    "                # probability using gaussian naive bayes method\n",
    "                v = x_test[i][x] # xth feature in the class\n",
    "                g = (1 / np.sqrt(2 * np.pi * std**2)) * np.exp(-((v-mean)**2)/(2*std**2))\n",
    "                prob[c] += np.log(g)\n",
    "           \n",
    "            # probability of the class\n",
    "            prob[c] += np.log((class_count + num_classes) / len(y_train))\n",
    "            \n",
    "        y_pred[0][i] = np.argmax(prob)\n",
    "                \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) [10 points] Calculate the accuracy and the F1 score of test data using both of your models implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Accuracy: 0.8892857142857142\n",
      "Multinomial F1: 0.8655097613882863\n",
      "Gaussian Accuracy: 0.5928571428571429\n",
      "Gaussian F1: 0.32941176470588235\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_actual,y_pred):\n",
    "    ## Your code here\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    \n",
    "    for i in range(len(y_actual)):\n",
    "        # TP: actual = 1, predicted = 1\n",
    "        if y_actual[i] == 1 and y_pred[0][i] == 1: \n",
    "            true_positive += 1\n",
    "       \n",
    "        # TN: actual = 0, predicted = 0\n",
    "        elif y_actual[i] == 0 and y_pred[0][i] == 0: \n",
    "            true_negative += 1\n",
    "        \n",
    "        # FP: actual = 0, predicted = 1\n",
    "        elif y_actual[i] == 0 and y_pred[0][i] == 1: \n",
    "            false_positive += 1\n",
    "        \n",
    "        # FN: actual = 1, predicted = 0\n",
    "        elif y_actual[i] == 1 and y_pred[0][i] == 0: \n",
    "            false_negative += 1\n",
    "            \n",
    "    return false_positive, false_negative, true_positive, true_negative\n",
    "\n",
    "classes = [0,1]\n",
    "\n",
    "# MULTINOMIAL \n",
    "multinom_y_pred = multinomial_nb_classifier(x_test, x_train, y_train, classes)\n",
    "\n",
    "fp, fn, tp, tn = evaluate(y_test, multinom_y_pred)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"Multinomial Accuracy: {accuracy}\")\n",
    "print(f\"Multinomial F1: {f1}\")\n",
    "\n",
    "# GAUSSIAN\n",
    "gaussian_y_pred = gaussian_nb_classifier(x_test, x_train, y_train, classes)\n",
    "\n",
    "fp, fn, tp, tn = evaluate(y_test, gaussian_y_pred)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"Gaussian Accuracy: {accuracy}\")\n",
    "print(f\"Gaussian F1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Questions\n",
    "You are given a training dataset in CSV format (hw4_cluster.csv). The files each contain 40 rows with 2 columns. Column 1 & 2 are the features. There are no labels for this dataset. Your goal for this assignment is to implement different clustering algorithms and run them on this dataset. For this assignment you can assume the distance function is the Euclidean distance.\n",
    "1) [35 points] Implement a generalized K-means/median algorithm. You should have a single function that takes in as input the data points, K, and some other hyperparameters, specified below. The function should return K sets of data points. Each set corresponding to one cluster.\n",
    "\n",
    "The hyperparameters your functions should support and the values they can take are:\n",
    "* The method for calculating the centroid: Means or Median\n",
    "* The initialization method: Random Split Initialization or Random Seed Selection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I left the bonus question unanswered but I started it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_Means, Random Split Initialization\n",
      "[[-0.595546  -1.4548001]\n",
      " [-1.2124729 -1.2988459]\n",
      " [ 3.8623366  4.2972736]\n",
      " [ 4.205326   4.9409523]\n",
      " [ 2.5081396  4.4804144]\n",
      " [ 3.1851852  4.6804123]\n",
      " [ 2.2065609  3.7271812]\n",
      " [ 2.637779   4.8631096]\n",
      " [ 3.3148954  6.9001   ]\n",
      " [ 4.326521   7.821588 ]\n",
      " [ 3.0818818  5.045526 ]\n",
      " [ 4.10377    5.597295 ]\n",
      " [ 4.0034175  5.902063 ]\n",
      " [ 8.99092   17.243813 ]]\n",
      "3\n",
      "[3.18705101 5.19614874]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import training set\n",
    "data = np.loadtxt(\"hw4_cluster.csv\", delimiter=',', skiprows=1)\n",
    "\n",
    "def k_means_median(data, num_clusters, max_its, method, initialization):\n",
    "    clusters = []\n",
    "    dimension = len(data[1])\n",
    "    final_buckets = np.zeros((num_clusters, len(data)//num_clusters))\n",
    "    \n",
    "    if method == \"means\":\n",
    "        if initialization == \"Random Split Initialization\":\n",
    "            clusters, means = random_split(data, num_clusters, method)\n",
    "            temp_distance = 0\n",
    "            min_distance = 0\n",
    "            print(clusters[0])\n",
    "            # print(means)\n",
    "            # print(clusters)\n",
    "            print(len(means))\n",
    "            print(means[0])\n",
    "            for i in range(len(means)):\n",
    "                centroid = means[i]\n",
    "                cluster = clusters[i]\n",
    "                for point_index in range(len(cluster)):\n",
    "                    point = cluster[point_index]\n",
    "                    temp_distance = distance(centroid, point)\n",
    "                    if temp_distance < min_distance:\n",
    "                        min_distance = temp_distance\n",
    "                        clusters\n",
    "            \n",
    "        elif initialization == \"Random Seed Selection Method\":\n",
    "            clusters = random_seed(data, num_clusters, method)\n",
    "            \n",
    "    elif method == \"median\":\n",
    "        if initialization == \"Random Split Initialization\":\n",
    "            clusters, medians = random_split(data, num_clusters, method)\n",
    "        elif initialization == \"Random Seed Selection Method\":\n",
    "            clusters = random_seed(data, num_clusters, method)\n",
    "            \n",
    "    return final_buckets\n",
    "\n",
    "def random_split(data, k, method):\n",
    "    random_indices = np.random.choice(range(0, k), replace = True, size = data.shape[0])\n",
    "    means_medians = []\n",
    "    clusters = []\n",
    "    for i in range(k):\n",
    "        if method == \"means\":\n",
    "            means_medians.append(np.mean(data[random_indices == i], axis=0)) # centroid\n",
    "        elif method == \"median\":\n",
    "            means_medians.append(np.median(data[random_indices == i], axis=0))\n",
    "        clusters.append(data[random_indices == i])\n",
    "\n",
    "    return clusters, means_medians\n",
    "\n",
    "# def random_seed(data, k, method):\n",
    "        \n",
    "def distance(point1, point2):\n",
    "    diff = point1 - point2\n",
    "    diff_squared = np.dot(diff.T, diff)\n",
    "    return (np.sqrt(diff_squared))\n",
    "\n",
    "\n",
    "k_means = \"means\"\n",
    "k_median = \"median\"\n",
    "rand_split = \"Random Split Initialization\"\n",
    "rand_seed = \"Random Seed Selection Method\"\n",
    "num_buckets = 3\n",
    "max_its = 5\n",
    "\n",
    "print(\"K_Means, Random Split Initialization\")\n",
    "print(k_means_median(data, num_buckets, max_its, k_means, rand_split))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(\"K_Median, Random Split Initialization\")\n",
    "# print(k_means_median(data, num_buckets, max_its, k_median, rand_split))\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"K_Means, Random Seed Selection Method\")\n",
    "# print(k_means_median(data, num_buckets, max_its, k_means, rand_seed))\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"K_Median, Random Seed Selection Method\")\n",
    "# print(k_means_median(data, num_buckets, max_its, k_median, rand_seed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# array = np.array([ [[1,2], [3,4], [5,6]], [[7,8],[9,10],[11,12]], [[7,8],[9,10],[11,12]] ])\n",
    "# dimension = 2\n",
    "# num_clusters = 3 # number of clusters\n",
    "# means = np.zeros((num_clusters, dimension))\n",
    "\n",
    "# temp = np.zeros_like(array)\n",
    "# print(temp)\n",
    "\n",
    "# for arr in array:\n",
    "#     # for each index in a point = 2 loops total\n",
    "#     for i in range(dimension):\n",
    "#         # for each cluster = 3 loops total\n",
    "#         for j in range(len(array[i])):\n",
    "#             test = arr[j][i]\n",
    "#             for t in temp:\n",
    "#                 t[i][j] = test\n",
    "#         print(np.mean(t[i][j]))\n",
    "\n",
    "# for j in range(num_clusters):\n",
    "#     # print(np.mean(temp[j]))\n",
    "#     means[j][i] = np.mean(temp[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
